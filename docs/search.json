[
  {
    "objectID": "posts/artificial-intelligences-blame-game/index.html",
    "href": "posts/artificial-intelligences-blame-game/index.html",
    "title": "Artificial Intelligence: The Blame Game",
    "section": "",
    "text": "therefore, reprehensibility. As the engineer, they have free will to leave the company, change the project, not kill the princess. However, the artificial agent does not. Because ascribing blame to an entity without psychological capacity is fruitless, maybe the question should therefore be how much ethical responsibility can we expect of our engineers? Are those who are taking the orders responsible for the problem? If they are the gateman, can we blame them? Or, if the artificial agent is the gateman, are they the Baron? And if so, are they responsible for our current culture or the implantation of a destructive model? Ultimately, deciding who to blame is nuanced and changes from situation to situation. But I think the process of breaking it down into its various parts illuminates a few key questions we should be thinking about as we continue down the AI rabbit hole. I think primarily it highlights how we should be empowering everyone throughout the process to keep ethical considerations at the back of their mind and encourage employees to be asking difficult questions. These questions lead to ethical explorations that are imperative to be considered if we want artificial intelligence to be a tool, not a hindrance. I, in no way, have the answer to the blame game, but I do have ideas on how we answer some of the key questions it raises. As I mentioned above, the domain-specific questions that come up when building an AI model which is meant to be implemented in a society with such complexity must be tackled by a team of multidisciplinary experts. No one person can be tasked to play the role of philosopher, psychologist, economist, scientist… In addition, it would be incredibly effective to empower employees within tech to be able to take the moral high ground and challenge their superiors, without the risk of job loss. Ideally, engineers would have a moral stop work authority. However, this is a tricky realm to navigate, and to put the responsibility solely on the employees excuses the billionaire CEOs of our world. In\naddition, it is problematic and lacks a nuanced understanding of the privileges of those who hold these various positions. Moreover, In addition to empowering engineers, I think policy is necessary to establish a protocol that holds executives accountable. Among all of these things, continued research into the expanding role of AI in society and a commitment to open science is imperative to the prevention of AI failure. I believe the collection of goals I’ve discussed above, while only scratching the surface, will not only lead to a more thoughtful, productive community but will work to clarify the blame game, and hopefully, prevent the need for assigning blame whenever possible.\n\n\n\n\n\nCitationBibTeX citation:@online{giesie2022,\n  author = {Mallory Giesie},\n  title = {Artificial {Intelligence:} {The} {Blame} {Game}},\n  date = {10/24/2022},\n  url = {https://mallorygiesue.github.io/posts/arctic-traffic-analysis/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMallory Giesie. 10AD–24AD. “Artificial Intelligence: The Blame\nGame.” 10AD–24AD. https://mallorygiesue.github.io/posts/arctic-traffic-analysis/."
  },
  {
    "objectID": "projects/arctic-traffic-analysis/index.html",
    "href": "projects/arctic-traffic-analysis/index.html",
    "title": "Analysis of Changing Arctic Traffic",
    "section": "",
    "text": "Introduction:\nThe Arctic Sea, once an area nearly impenetrable, has become a new playground for shipping and tourism alike. This is impart due to advances in ships and navigation allowing safer passage to areas once completely isolated. More importantly, and what this analysis is meant to reflect, is the effect climate change has had on ice extent allowing for more ships to voyage, more months out of the year throughout the arctic. It has been reported that between the years 2013 and 2019 the number of ships entering arctic grew by 25%. (PAME). In 2017, The Polar Code was enacted by the International Maritime Organization providing framework for safely navigating polar waters while adhering to environmental protocol. The region is under a time of immense change, and understanding ship traffic patterns is integral to assessing potential increased destruction to the area.\nMy Data\nNorth Pacific and Arctic Marine Vessel Traffic Dataset (2015-2020); Hexagon Data\nTo frame my question I will primarily be using satellite collected data tracking unique ship observations in the Pacific Arctic sector which extends from the Aleutian Islands through Bering Sea and into the southern Chukchi and Beaufort Seas.\nThe data was collected monthly between January 1, 2015 and December 31, 2020. To calculate shipping intensity, they used an Automatic Identification System data collected from all ships over 300 gross tonnes on an international voyage, all cargo ships over 500 gross tonnes, and all passenger ships. They then converted the AIS data into a monthly hex data set which I downloaded as vector data. I chose to focus on the monthly sum of unique ships (both cargo and passenger) in the region.\nVisualized Hex Data\n\n\nCode\n`2015-01` <- `2015-01` |>  mutate_all(~replace(., is.na(.), 0))\n  \n`2015-04` <- `2015-04` |> mutate_all(~replace(., is.na(.), 0))\n  \n`2015-07` <- `2015-07` |> mutate_all(~replace(., is.na(.), 0))\n  \n`2015-10` <- `2015-10` |>  mutate_all(~replace(., is.na(.), 0))\n  \n\njan <- tm_shape(`2015-01`) +\n  tm_polygons(\n    col = \"nMMSI_A\",\n    palette = \"viridis\",\n    title = \"Number of Unique Ships\",\n    breaks = c(0,1,100,200,300,400,500)\n  ) + tm_layout(legend.outside = TRUE, title = \"January 2015\")\n\n\napr <- tm_shape(`2015-04`) +\n  tm_polygons(\n    col = \"nMMSI_A\",\n    palette = \"viridis\",\n    title = \"Number of Unique Ships\",\n    breaks = c(0,1,100,200,300,400,500)\n  ) + tm_layout(legend.outside = TRUE, title = \"April 2015\")\n\njuly  <- tm_shape(`2015-07`) +\n  tm_polygons(\n    col = \"nMMSI_A\",\n    palette = \"viridis\",\n    title = \"Number of Unique Ships\",\n    breaks = c(0,1,100,200,300,400,500)\n  ) + tm_layout(legend.outside = TRUE, title = \"July 2015\")\n\noct <- tm_shape(`2015-10`) +\n  tm_polygons(\n    col = \"nMMSI_A\",\n    palette = \"viridis\",\n    title = \"Number of Unique Ships\",\n    breaks = c(0,1,100,200,300,400,500)\n  ) + tm_layout(legend.outside = TRUE, title = \"October 2015\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClimate Bulletin: Arctic Sea Ice Extent Anomalies\nI also utilized sea ice anomaly data from Copernicus Climate Change data base which uses ECMWF’s (European Center for Medium Weather Forecasts) ERA-Interim reanalysis of observations. For the data I utilized the calculation for absolute sea ice area (SIA) anomaly in millions sq km.\nMy Analysis plan\n1. Time Series Analysis\nI wanted to look at the changes in ship traffic over time and how these may change given the part of region of the data set we were looking at. To do this I split my data up into north and south and ran decomposition to understand the underlying trends. I also ran this on the sea ice anomaly data (see appendix).\n2. OLS Linear regression\nI wanted to understand what factors determine the number of ships sited in a given month. To do this I summed each hex value to determine the number of vessels registered per month. I split these up to north and south and ran OLS linear regression with sea ice anomaly, year, and season (first or last half of the year) as my predicting variables. I then combined the data sets and ran an OLS linear regression which now included region (north or south) as a predictor of monthly ship numbers. Then finally I included an interaction to see if the effect of the season depends on the region.\nMy Results\nDecomposition\n\n\nCode\nall_years<- do.call(rbind, df_geom) |>\n  tibble::rownames_to_column(\"Cycle\") |>\n  mutate(hexID = str_sub(Cycle, 9)) |>\n  mutate(Cycle = str_sub(Cycle, 1, 7)) |> \n  select(-hexID) |>\n  pivot_wider(names_from = 'Cycle',\n              values_from = 'nMMSI_A') |> \n  mutate_all(~replace(., is.na(.), 0))\n\nsouth <- all_years |> slice(0:3500)\nnorth <- all_years |> slice(3501:6553)\nsouth <- st_drop_geometry(south)\nyearly_totals <- colSums(south, na.rm=T)\ntraffic_ts_south <- ts(yearly_totals, frequency=4, start=c(2015,1))\n\n#plot.ts(traffic_ts_south)\n\ntraffic_ts_south <- as_tsibble(traffic_ts_south)\n\nsouth_time <- traffic_ts_south  %>%\n  model(\n    classical_decomposition(value, type = \"additive\")\n  ) %>%\n  components() %>%\n  autoplot() +\n  labs(title = \"Decomposition of Ship Traffic: Bering Sea from 2015-2021\")\n\nnorth <- st_drop_geometry(north)\nyearly_totals <- colSums(north, na.rm=T)\ntraffic_ts_north<- ts(yearly_totals, frequency=4, start=c(2015,1))\n#plot.ts(traffic_ts_north)\n\ntraffic_ts_north <- as_tsibble(traffic_ts_north)\n\nnorth_time <- traffic_ts_north  %>%\n  model(\n    classical_decomposition(value, type = \"additive\")\n  ) %>%\n  components() %>%\n  autoplot() +\n  labs(title = \"Decomposition of Ship Traffic: Northern Bering and Southern Chukchi\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom these two figures you can see the different ways seasonal and yearly trends play in the northern and southern regions of our data set. The number of ships seen in the northern region, is much more dependent on the season than the southern. This makes sense, because the ice is much denser in the earlier half of the year. We can see from the visualization of our data, it is rare to see many ships travel far north in January and April. This also plays into the intuition why the yearly trend for the northern portion of the data is stronger. As sea ice melts, the area where we will see more and more ship traffic will be the northern polar areas where it was once barricaded by ice.\nMultiple Linear Regression\n\n\nCode\njuly_anomaly <- read.csv(file.path(rootdir, \"july_anomaly.csv\")) |> slice(37:42) |> dplyr::select(Date, X.SIA.anomaly)\noct_anomaly <- read.csv(file.path(rootdir, \"oct_anomaly.csv\")) |> slice(37:42) |> dplyr::select(Date, X.SIA.anomaly)\njanuary_anomaly <- read.csv(file.path(rootdir, \"january_anomaly.csv\")) |> slice(37:42) |> dplyr::select(Date, X.SIA.anomaly)\napril_anomaly <- read.csv(file.path(rootdir, \"april_anomaly.csv\")) |> slice(37:42) |> dplyr::select(Date, X.SIA.anomaly)\n\nanomaly_south <- rbind(july_anomaly,oct_anomaly,april_anomaly,january_anomaly) |> arrange(Date) |> cbind(traffic_ts_south) |> dplyr::select(-index) |> dplyr::rename(Extent = X.SIA.anomaly, Value = value) |> mutate(Date = lubridate::ym(Date)) |> mutate(Year = lubridate::year(Date), Month = lubridate::month(Date)) |> mutate(Month = as.factor(Month)) |> mutate(Season = ifelse(Month == 1 | Month == 4, \"First_Half\", \"Last_half\")) |> mutate(Season = as.factor(Season))\n\nanomaly_north <- rbind(july_anomaly,oct_anomaly,april_anomaly,january_anomaly) |> arrange(Date) |> cbind(traffic_ts_north) |> dplyr::select(-index) |> dplyr::rename(Extent = X.SIA.anomaly, Value = value) |> mutate(Date = lubridate::ym(Date)) |> mutate(Year = lubridate::year(Date), Month = lubridate::month(Date)) |> mutate(Month = as.factor(Month)) |> mutate(Season = ifelse(Month == 1 | Month == 4, \"First_Half\", \"Last_half\")) |> mutate(Season = as.factor(Season))\n\n\nTo begin, I ran regression to predict monthly ship observations in the southern region of my data set (see appendix). The only predictor that had a significant effect on amount of ships seen monthly in the southern region of our data set was year. The intercept was nonsensical but the model predicts that each year ship sightings grow by an estimated 1,492 ships.\nI then ran regression to predict monthly ship observations in the northern region as you can see below.\n\\[\nNorthernShipCount = \\beta_0 + \\beta_1 * Year_i + \\beta_1 * Season_i + \\beta_1 * Ice Extent + \\epsilon_i\n\\]\n\n\nCode\nmod1 <- lm(Value ~ abs(Extent) + Year + Season, data = anomaly_north)\nmod1 %>%\n  tbl_regression(intercept= TRUE) %>%\n  as_gt() %>%\n  gt::tab_source_note(gt::md(\"*Kelly Kapsar, Benjamin Sullender, and Aaron Poe. 2022. North Pacific and Arctic Marine Vessel Traffic Dataset (2015-2020); Hexagon Data.*\"))\n\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n-2,326,423\n-3,416,236, -1,236,609\n<0.001\n    abs(Extent)\n-2,346\n-4,719, 26\n0.052\n    Year\n1,156\n615, 1,696\n<0.001\n    Season\n\n\n\n        First_Half\n—\n—\n\n        Last_half\n15,338\n13,260, 17,416\n<0.001\n  \n  \n    \n      Kelly Kapsar, Benjamin Sullender, and Aaron Poe. 2022. North Pacific and Arctic Marine Vessel Traffic Dataset (2015-2020); Hexagon Data.\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nAdjusted R-squared: 0.929014\nInterpretation\nCoefficients: Unlike the southern region, the northern region has more significant predictors. This makes sense from what we saw in our time series decomposition. For each unit increase in ice extent (1 million sq km farther from the mean) the model predicts to see 2,346 more ships holding year fixed and season fixed. For each year increase, holding extent and season fixed, ship sightings grow by an estimated 2,346. And finally, the last half of the year adds a predicted increase of 15,338 ship sightings compared to the beginning half of the year holding fixed the year and ice extent. Next I combined my southern and northern observations. And ran regression with all of my predictors and no interactions (See appendix)\n\n\nCode\nanomaly_south <- anomaly_south |> mutate(Region = \"south\")\nanomaly_north <-  anomaly_north |> mutate(Region = \"north\")\nall_anomaly_dat <- rbind(anomaly_south, anomaly_north) |> mutate(Region = as.factor(Region))\n\n\nUnderstanding the effect season has on the each region\n\\[\nTotalShipCount = \\beta_0 + \\beta_1 * Region_i + \\beta_1 * Season_i +\n\\beta_3 * Season_i * Region_i + \\epsilon_i\n\\]\n\n\nCode\nmod3 <- lm(Value ~ Region +Season + Region:Season, data = all_anomaly_dat)\n\nmod3 %>%\n  tbl_regression(intercept= TRUE) %>%\n  as_gt() %>% \n  gt::tab_source_note(gt::md(\"*Kelly Kapsar, Benjamin Sullender, and Aaron Poe. 2022. North Pacific and Arctic Marine Vessel Traffic Dataset (2015-2020); Hexagon Data.*\"))\n\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n3,867\n523, 7,212\n0.024\n    Region\n\n\n\n        north\n—\n—\n\n        south\n58,290\n53,560, 63,020\n<0.001\n    Season\n\n\n\n        First_Half\n—\n—\n\n        Last_half\n14,255\n9,525, 18,985\n<0.001\n    Region * Season\n\n\n\n        south * Last_half\n-16,002\n-22,691, -9,312\n<0.001\n  \n  \n    \n      Kelly Kapsar, Benjamin Sullender, and Aaron Poe. 2022. North Pacific and Arctic Marine Vessel Traffic Dataset (2015-2020); Hexagon Data.\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nAdjusted R-squared: 0.9529827\nCoefficients: The intercept: 3,867 ships is the predicted number of ship sightings in the northern region in the first half of the year. 58,290 is the difference in predicted ship sightings between the north and south. 14,255 is the predicted difference in ship sightings between the first and last half of the year. From the interaction coefficient, if we wanted to determine the number of predicted ship sightings in the south in the last half of the year we would calculate\n\\[ShipSightings = 3,867 + 58,290 + 14,255 - 16,002\\] \\[ 60,410 \\] An adjusted R-squared value of .9529 means that season and region alone contribute to about 95% percent of the variability in ship sightings.\nFinally, I wanted to see the strength of the model with all of my predictors and the interaction between season and region.\nPutting it all together\n$$\nTotalShipCount = _0 + _1 * Year_i + _1 * Season_i + _1 * IceExtent_i + _3 * Season_i * Region_i + _i\n$$\n\n\nCode\nmod4 <- lm(log(Value) ~ Extent + Year + Region +Season + Region:Season, data = all_anomaly_dat)\n\nmod4 %>%\n  tbl_regression(intercept= TRUE) %>%\n  as_gt() %>% \n  gt::tab_source_note(gt::md(\"*Kelly Kapsar, Benjamin Sullender, and Aaron Poe. 2022. North Pacific and Arctic Marine Vessel Traffic Dataset (2015-2020); Hexagon Data.*\"))\n\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n-187\n-274, -101\n<0.001\n    Extent\n0.21\n0.02, 0.39\n0.032\n    Year\n0.10\n0.05, 0.14\n<0.001\n    Region\n\n\n\n        north\n—\n—\n\n        south\n2.9\n2.7, 3.1\n<0.001\n    Season\n\n\n\n        First_Half\n—\n—\n\n        Last_half\n1.7\n1.5, 2.0\n<0.001\n    Region * Season\n\n\n\n        south * Last_half\n-1.7\n-2.0, -1.4\n<0.001\n  \n  \n    \n      Kelly Kapsar, Benjamin Sullender, and Aaron Poe. 2022. North Pacific and Arctic Marine Vessel Traffic Dataset (2015-2020); Hexagon Data.\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nAdjusted R-Squared: 0.9611927\nCoefficients: The residuals of this final model were closer to a normal distribution when ship sightings was logged. Th e interpretation of the coefficients reflects that.\nThe intercept is nonsensical due to the nature of our year variable. From the model, we can see that sea ice extent has an effect on the amount of ships observed in our data set. A 1% change in sea ice extent increases ships by 21%. The south has 94% more ship sightings than the north. the last half of the years adds a predicted 81% more ship sightings than the north.\nInterpretation and Conclusion:\nAs we can see year and sea ice extent are significant predictors, but the inclusion of them in the model only increases the R-squared by about 1%. Based on this model and the above analysis, I feel confident concluding that arctic ship traffic in the northern region of my data set is undergoing change and is more vulnerable to the effects of sea ice decline than the southern portion. This makes sense as throughout the entirety of the year, you still see ship observations in the south, and ship navigation is not determined by ice extent, as it is in the northern region.\nLimitations:\nOne limitation may come from interpreting my models. In order to conduct my analysis, I totaled ship observations in each area. Ship observations between hexagons naturally includes the same vessels. This means when one interprets the predicted values values of my model 58,290 ship observations in the south doesn’t mean 58,290 ships, but rather times a ship was seen, regardless of if it was already registered. Also, for my sea ice extent anomaly data I only had a holistic calculation for the entirety of the arctic, not the specific region my ship traffic focused on. For this I was assuming ice extent trends for the arctic where close enough to the actual numbers in my region, which could have affected the accuracy of my models. In addition, it is also difficult to promise that my predictors are exogenous.\nFuture Research:\nI think it would be interesting to dig deeper into the northern region and target which areas are seeing a large increase in ship observations. To do this it may be better to have ice extent data that apply to more specific regions. I would also suggest looking at specific key months to see how they are changing. For example, if the season that ice becomes too thick to travel is November, it would be interesting to look at how that month is changing over time too understand how fast season extensions are affecting ship traffic.\nGithub Repository\nAppendix\nSupporting Figures\n\n\nCode\nanomaly <- all_anomaly_dat |> select(Extent)\nanomaly_ts <- ts(anomaly, frequency=4, start=c(2015,1))\n\nanomaly_ts <- as_tsibble(anomaly_ts)\n\nanomaly_ts  %>%\n  model(\n    classical_decomposition(value, type = \"additive\")\n  ) %>%\n  components() %>%\n  autoplot() +\n  labs(title = \"Decomposition of Sea Ice Extent Anomalies in the Arctic 2015-2021\")\n\n\n\n\n\n\n\n\n\nCode\nmod <- lm(Value ~ Year, data = anomaly_south)\nmod %>%\n  tbl_regression(intercept= TRUE) %>%\n  as_gt() %>%\n  gt::tab_source_note(gt::md(\"*Kelly Kapsar, Benjamin Sullender, and Aaron Poe. 2022. North Pacific and Arctic Marine Vessel Traffic Dataset (2015-2020); Hexagon Data.*\"))\n\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n-2,948,149\n-6,551,902, 655,604\n0.10\n    Year\n1,492\n-295, 3,278\n0.10\n  \n  \n    \n      Kelly Kapsar, Benjamin Sullender, and Aaron Poe. 2022. North Pacific and Arctic Marine Vessel Traffic Dataset (2015-2020); Hexagon Data.\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\n\nCode\nmod2 <- lm(Value ~ Season + Year + Region + Extent, data = all_anomaly_dat)\nmod2 %>%\n  tbl_regression(intercept= TRUE) %>%\n  as_gt() %>%\n  gt::tab_source_note(gt::md(\"*Kelly Kapsar, Benjamin Sullender, and Aaron Poe. 2022. North Pacific and Arctic Marine Vessel Traffic Dataset (2015-2020); Hexagon Data.*\"))\n\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n-3,047,874\n-5,432,428, -663,321\n0.013\n    Season\n\n\n\n        First_Half\n—\n—\n\n        Last_half\n8,136\n3,589, 12,683\n<0.001\n    Year\n1,516\n333, 2,698\n0.013\n    Region\n\n\n\n        north\n—\n—\n\n        south\n50,289\n46,425, 54,153\n<0.001\n    Extent\n4,076\n-1,115, 9,266\n0.12\n  \n  \n    \n      Kelly Kapsar, Benjamin Sullender, and Aaron Poe. 2022. North Pacific and Arctic Marine Vessel Traffic Dataset (2015-2020); Hexagon Data.\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\n\nResidual Distributions of my models\n\n\n\n\nCode\nmod <- lm(Value ~ Year, data = anomaly_south)\n\nres <- resid(mod)\n\ninteraction_errors <- ggplot(data = mod, aes(sample = res)) +\n  geom_qq() +\n  geom_qq_line() +\n  ggtitle(\"Southern region residuals\")\n\ninteraction_errors\n\n\n\n\n\n\n\nCode\nmod3 <- lm(Value ~ Region +Season + Region:Season, data = all_anomaly_dat)\n\nres <- resid(mod3)\n\ninteraction_errors <- ggplot(data = mod3, aes(sample = res)) +\n  geom_qq() +\n  geom_qq_line() +\n    ggtitle(\"All data: season * region residuals\")\n\ninteraction_errors\n\n\n\n\n\n\n\nCode\nmod2 <- lm(Value ~ Season + Year + Region + Extent, data = all_anomaly_dat)\n\nres <- resid(mod2)\n\ninteraction_errors <- ggplot(data = mod2, aes(sample = res)) +\n  geom_qq() +\n  geom_qq_line() +\n    ggtitle(\"All data: all predictors residuals\")\n\ninteraction_errors\n\n\n\n\n\n\n\n\n\nCode\nmod1 <- lm(Value ~ abs(Extent) + Year + Season, data = anomaly_north)\nres <- resid(mod1)\n\ninteraction_errors <- ggplot(data = mod1, aes(sample = res)) +\n  geom_qq() +\n  geom_qq_line() +\n    ggtitle(\"Northern region residuals\")\n\ninteraction_errors\n\n\n\n\n\n\n\nCode\nmod4 <- lm(log(Value) ~ Extent + Year + Region +Season + Region:Season, data = all_anomaly_dat)\nres <- resid(mod4)\n\ninteraction_errors <- ggplot(data = mod4, aes(sample = res)) +\n  geom_qq() +\n  geom_qq_line() +\n  ggtitle(\"All Data: All predictors + region * season residuals\")\n\ninteraction_errors\n\n\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{giesie2022,\n  author = {Mallory Giesie},\n  title = {Analysis of {Changing} {Arctic} {Traffic}},\n  date = {10/24/2022},\n  url = {https://mallorygiesue.github.io/posts/arctic-traffic-analysis/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMallory Giesie. 10AD–24AD. “Analysis of Changing Arctic\nTraffic.” 10AD–24AD. https://mallorygiesue.github.io/posts/arctic-traffic-analysis/."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Blog",
    "section": "",
    "text": "Statistics for the Environment\n\n\nR\n\n\nMEDS\n\n\n\nTime Series Analysis of Changing Arctic Traffic from 2015 to 2020\n\n\n\nMallory Giesie\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mallory Giesie",
    "section": "",
    "text": "I’m Mallory Giesie.\nI am currently pursuing my Master’s of Environmental Data Science at the Bren School of Environmental Science & Management. For my master’s capstone my team will be working to use meteorological data in order to develop various climate scenarios which will be used to model future fire regimes in the California Central Coast. I made my way to Santa Barbara after completing my Bachelor’s degree at the American University of Paris in Quantitative Environmental Science. For my undergraduate thesis I simulated and analyzed a co-evolution experiment that I was lucky enough to assist on at the University of Konstanz. My goal is to continue to use data to assist in environmental exploration and problem solving. I’ll be updating this website as I continue throughout my journey!"
  },
  {
    "objectID": "ethics_blog.html",
    "href": "ethics_blog.html",
    "title": "The Blame Game and Artifical Intelligence",
    "section": "",
    "text": "As artificial intelligence becomes increasingly ubiquitous in our society, so do the various mistakes models and agents make, introducing the 21st-century blame game. The decisions the creators of artificial intelligence projects must make expand well beyond that of an engineer and raise complex philosophical, scientific, and socio-economic questions that if answered poorly, create damaging effects. Given the complexity of the job and the various people assigned at all levels to implement AI projects comes an important question: Who do we blame when things go awry? And if we establish these protocols, will it change how often and why these mistakes are made?\nAs I was reflecting on this question, I remembered a story I read that deals with many of these underlying principles. The story, titled The Drawbridge, reminded me of the inherent complexity of assigning blame when an AI agent or model fails. In the story, a jealous baron instructs the baroness not to leave their castle while he is gone. After he leaves, and as she grows lonely, she decides to visit her lover, believing that she will surely be home before her husband. On her way back she is greeted by a gateman who tells her if she crosses the bridge, she will be killed, as he has been ordered to do so by the king. Following this order from the gateman, she seeks help from a lover, a friend, and a boatman who all fail her in one way or another. The lover by saying their relationship is only sexual, and he cannot help her. The friend disapproves of the baroness defying her husband and is unwilling to help. And the boatman who demands payment for his service despite the baroness being penniless. Exhausting all of her options, she attempts to cross the bridge against the gateman’s orders and is killed.\nThis leaves the question, who is responsible for the baroness’s death? Is it the husband who commanded the order? Could it be the gateman for following orders? Is it her friend or lover who refused to help in her time of need? Or finally, is it the baroness herself for willingly putting herself in the position that led to her death? There is, of course, not a clear answer, and arguably all characters share in some blame for the death of the Baroness. I’d argue that the convoluted nature of the story and specifically the theme of blame can be applied to our society as we grapple with the effects of artificial intelligence all around us, unable to pinpoint who to thank or blame.\nThese characters and their real-world representations can be assigned to many variations of our AI blame game. In the context of AI, the baron could represent big tech. The castle: Silicon Valley. The baroness: society, shackled by technology and its choke hold over our lives but without the power to change it. The gateman enforcing the rules could represent an artificial agent itself, a blind follower of orders, rules, the algorithm. Alternatively, is the gateman the engineer working for a tech giant whose actions as a result of their time, skill, and or obedience are ultimately destructive? In these two tech world representations of the gateman, the engineer, or the artificial agent, there are vast differences in authority and therefore, reprehensibility. As the engineer, they have free will to leave the company, change the project, not kill the princess. However, the artificial agent does not. Because ascribing blame to an entity without psychological capacity is fruitless, maybe the question should therefore be how much ethical responsibility can we expect of our engineers? Are those who are taking the orders responsible for the problem? If they are the gateman, can we blame them? Or, if the artificial agent is the gateman, are they the Baron? And if so, are they responsible for our current culture or the implantation of a destructive model?\nUltimately, deciding who to blame is nuanced and changes from situation to situation. But I think the process of breaking it down into its various parts illuminates a few key questions we should be thinking about as we continue down the AI rabbit hole. I think primarily it highlights how we should be empowering everyone throughout the process to keep ethical considerations at the back of their mind and encourage employees to be asking difficult questions. These questions lead to ethical explorations that are imperative to be considered if we want artificial intelligence to be a tool, not a hindrance.\nI, in no way, have the answer to the blame game, but I do have ideas on how we answer some of the key questions it raises. As I mentioned above, the domain-specific questions that come up when building an AI model which is meant to be implemented in a society with such complexity must be tackled by a team of multidisciplinary experts. No one person can be tasked to play the role of philosopher, psychologist, economist, scientist… In addition, it would be incredibly effective to empower employees within tech to be able to take the moral high ground and challenge their superiors, without the risk of job loss. Ideally, engineers would have a moral stop work authority. However, this is a tricky realm to navigate, and to put the responsibility solely on the employees excuses the billionaire CEOs of our world. In addition, it is problematic and lacks a nuanced understanding of the privileges of those who hold these various positions. Moreover, In addition to empowering engineers, I think policy is necessary to establish a protocol that holds executives accountable. Among all of these things, continued research into the expanding role of AI in society and a commitment to open science is imperative to the prevention of AI failure. I believe the collection of goals I’ve discussed above, while only scratching the surface, will not only lead to a more thoughtful, productive community but will work to clarify the blame game, and hopefully, prevent the need for assigning blame whenever possible."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Blog",
    "section": "",
    "text": "Ethics\n\n\nAI\n\n\nMEDS\n\n\n\nHow are we to assign blame when an artifical agent fails? A ramble..\n\n\n\nMallory Giesie\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  }
]